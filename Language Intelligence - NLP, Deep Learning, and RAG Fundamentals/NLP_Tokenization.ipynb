{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Tokenization with TensorFlow Keras\n",
        "### Overview\n",
        "\n",
        "In Natural Language Processing (NLP), Tokenization is the process of splitting text into smaller units called tokens — usually words or subwords.\n",
        "This makes it easier for a machine to understand and process text data.\n",
        "\n",
        "In this example, I’ll use **TensorFlow’s Keras Tokenizer** to break a few sentences into **tokens** and create a **word index** — a `dictionary` that maps each unique word to a `numerical value`."
      ],
      "metadata": {
        "id": "Ut9HFZeiVje_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow\n",
        "An end-to-end open source machine learning platform.\n",
        "\n",
        "### Keras\n",
        "Keras is a deep learning API designed for human beings, not machines. Keras focuses on debugging speed, code elegance & conciseness, maintainability, and deployability. When you choose Keras, your codebase is smaller, more readable, easier to iterate on.\n",
        "\n",
        "### Tokenizer\n",
        "I import the Tokenizer class from Keras.\n",
        "This class helps to convert text into sequences of numbers that a neural network can understand."
      ],
      "metadata": {
        "id": "WiQl2ckzXlv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Defining the Sentences\n",
        "sentences = [\n",
        "    'i love my cat',\n",
        "    'I, love my dog',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXvOERFEWKnX",
        "outputId": "d8bc86ff-1667-4e02-ec2b-dbce3f968853"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have three short text samples.\n",
        "Notice that they include differences in:\n",
        "\n",
        "* Capitalization (i vs I)\n",
        "\n",
        "* Punctuation (, and !)\n",
        "\n",
        "The Tokenizer will handle these automatically by:\n",
        "\n",
        "* Converting text to lowercase\n",
        "\n",
        "* Ignoring punctuation\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Creating the Tokenizer Object\n",
        "\n",
        "`tokenizer = Tokenizer(num_words=100)`\n",
        "\n",
        "This line creates a **Tokenizer** and tells it to keep the **top 100 most frequent words** (based on how often they appear in your dataset).\n",
        "Since my dataset is small, it will keep all words.\n",
        "\n",
        "\n",
        "---\n",
        "### Fitting the Tokenizer on Texts\n",
        "\n",
        "`tokenizer.fit_on_texts(sentences)`\n",
        "\n",
        "\n",
        "This step **analyzes** all sentences, finds every **unique word**, and assigns each one a **unique index number.**\n",
        "\n",
        "For example:\n",
        "\n",
        "The most frequent word gets index 1\n",
        "\n",
        "The next most frequent gets 2, and so on.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Viewing the Word Index\n",
        "\n",
        "`word_index = tokenizer.word_index`\n",
        "\n",
        "`print(word_index)`\n",
        "\n",
        "\n",
        "This prints a dictionary showing the mapping from each word to its unique integer ID.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Output:\n",
        "\n",
        "{'love': 1, 'my': 2, 'dog': 3, 'i': 4, 'cat': 5, 'you': 6}\n",
        "\n",
        "\n",
        "Break that down:\n",
        "\n",
        "* 'love' → 1 → most frequent word (appears in all sentences)\n",
        "\n",
        "* 'my' → 2 → appears in every sentence\n",
        "\n",
        "* 'dog' → 3\n",
        "\n",
        "* 'i', 'cat', 'you' follow in frequency order\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3_lSrPvNeTeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Happens Behind the Scenes\n",
        "\n",
        "When `fit_on_texts()` runs, it:\n",
        "\n",
        "1. Cleans the text: removes punctuation, lowercases everything.\n",
        "\n",
        "2. Splits sentences into words (tokens).\n",
        "\n",
        "3. Counts word frequencies.\n",
        "\n",
        "4. Assigns indices based on how frequent each word is.\n",
        "\n",
        "This dictionary (word_index) can then be used to:\n",
        "\n",
        "* Convert text to sequences (for input into a model)\n",
        "\n",
        "* Understand which words are most common\n",
        "\n",
        "* Build embeddings or vocabularies for neural networks"
      ],
      "metadata": {
        "id": "8974KQH1gRuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### ✨ Summary\n",
        "| Step                   | Purpose                                    |\n",
        "| ---------------------- | ------------------------------------------ |\n",
        "| **Tokenizer creation** | Define how many words to keep              |\n",
        "| **fit_on_texts()**     | Learn all unique words and assign indices  |\n",
        "| **word_index**         | Get a mapping of each word → integer       |\n",
        "| **Result**             | Ready-to-use numerical data for NLP models |\n",
        "\n"
      ],
      "metadata": {
        "id": "SBJJ5NMcgjEj"
      }
    }
  ]
}