{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“ Homework: Building Your First NLP Pipeline\n",
        "\n",
        "**Theory & Elaboration**\n",
        "This homework is your capstone for the week. You'll combine all the concepts we've discussed into a single, functional **NLP Pipeline**. This is one of the most common and essential tasks in all of Natural Language Processing.\n",
        "\n",
        "* **The NLP Pipeline (An Assembly Line for Text) ðŸ­**: Think of a pipeline as an assembly line for raw text. Messy, unstructured sentences go in one end, and each step on the line performs a specific, targeted transformation. The output of one step becomes the input for the next, until clean, structured, and useful data emerges at the end. The quality of your pipeline directly impacts the quality of your final analysis.\n",
        "\n",
        "* **The Goal: Preparing for a \"Bag of Words\"**: The ultimate goal of this cleaning process is to prepare our text for a machine learning model. The output of your pipelineâ€”a clean list of root wordsâ€”is the perfect input for a foundational model called the **Bag of Words (BoW)**.\n",
        "    * **Analogy**: Imagine you take a book, tear out all the pages, cut out every single word, and throw them all into a giant bag. You then shake the bag, disregarding all grammar and sentence order. Finally, you create a frequency count of every unique word. That's a Bag of Words!\n",
        "    * This simple frequency model is surprisingly powerful and is the basis for tasks like **document classification** (e.g., spam vs. not spam) and **sentiment analysis**. Your pipeline is the essential first step to creating this model.\n",
        "\n",
        "* **Why Order Matters**: As an engineer, the sequence of your pipeline is a critical design choice. A logical order is: `Lowercase -> Tokenize -> Filter -> Normalize`. This ensures, for example, that you lowercase words *before* checking them against an all-lowercase stop word list.\n",
        "\n",
        "**Your Challenge**\n",
        "Your task is to build a function, `preprocess_text`, that accepts a raw string of text. It must perform a series of normalization steps and return a list of cleaned, stemmed tokens, ready for a Bag of Words model.\n",
        "\n",
        "**Requirements & Hints:**\n",
        "* Your final list of tokens should be **lowercased**.\n",
        "* It should **not** contain any **punctuation**.\n",
        "* It should **not** contain any **stop words**.\n",
        "* Each word in the final list should be **stemmed** to its root form.\n",
        "* **Tools**: You will need to import and use `word_tokenize`, `stopwords` from `nltk`, and `string` from Python. You'll also need to initialize the `PorterStemmer`.\n",
        "* **Advanced Tip**: For a more elegant solution, see if you can perform the filtering and stemming steps inside a single list comprehension.\n",
        "\n",
        "**Bonus Challenge ðŸŒŸ**\n",
        "Create a second function called `preprocess_with_lemma` that performs the same cleaning steps but uses **spaCy's lemmatization** instead of NLTK's stemming for the final normalization step.\n",
        "* **Hint**: You'll need to process the text with a spaCy `nlp` object to access the `.lemma_` attribute of each token. How can you integrate this with your existing filtering logic for stop words and punctuation?"
      ],
      "metadata": {
        "id": "BNQtyqftK8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqQ74UcdHuIc",
        "outputId": "36b0487c-0e4f-41d3-c903-9538d407a0f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/12.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/12.8 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/12.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Import necessary libraries ---\n",
        "import nltk\n",
        "import string\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# --- Setup tools ---\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = set(string.punctuation)\n",
        "ps = PorterStemmer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    This function takes raw text and returns a list of cleaned, stemmed tokens.\n",
        "    \"\"\"\n",
        "    # 1ï¸âƒ£ Convert to lowercase and tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 2ï¸âƒ£ Filter out stopwords and punctuation, then stem remaining words\n",
        "    cleaned_tokens = [\n",
        "        ps.stem(word)\n",
        "        for word in tokens\n",
        "        if word not in stop_words and word not in punctuations\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "def preprocess_with_lemma(text):\n",
        "    \"\"\"\n",
        "    This function takes raw text and returns a list of cleaned, lemmatized tokens.\n",
        "    \"\"\"\n",
        "    # 1ï¸âƒ£ Process text with spaCy\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    # 2ï¸âƒ£ Keep only non-stopword, non-punctuation lemmas\n",
        "    cleaned_tokens_lemma = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens_lemma\n",
        "\n",
        "\n",
        "# --- Testing functions ---\n",
        "test_text = \"Data Science is an amazing field! But, you'll need to clean your data first before analysis.\"\n",
        "\n",
        "# Test the main function\n",
        "processed_tokens = preprocess_text(test_text)\n",
        "print(\"--- Main Task (Stemming) ---\")\n",
        "print(\"Original Text:\", test_text)\n",
        "print(\"Processed Tokens:\", processed_tokens)\n",
        "\n",
        "# Test the bonus function\n",
        "processed_lemma_tokens = preprocess_with_lemma(test_text)\n",
        "print(\"\\n--- Bonus Challenge (Lemmatization) ---\")\n",
        "print(\"Processed Tokens:\", processed_lemma_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuKwQBqYLDa3",
        "outputId": "69f266e9-f477-43de-9990-512e128cc151"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Main Task (Stemming) ---\n",
            "Original Text: Data Science is an amazing field! But, you'll need to clean your data first before analysis.\n",
            "Processed Tokens: ['data', 'scienc', 'amaz', 'field', \"'ll\", 'need', 'clean', 'data', 'first', 'analysi']\n",
            "\n",
            "--- Bonus Challenge (Lemmatization) ---\n",
            "Processed Tokens: ['data', 'science', 'amazing', 'field', 'need', 'clean', 'datum', 'analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… Self-Assessment\n",
        "\n",
        "Run the cell below to check your work. This script will evaluate the key exercises and the final homework to provide a score and detailed feedback. Make sure you have run all the cells above this one first."
      ],
      "metadata": {
        "id": "l8GKOdq2LTDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell to check your work\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def check_week5_nlp_tasks():\n",
        "    \"\"\"Checks the student's work for Week 5 and provides feedback.\"\"\"\n",
        "    score = 0\n",
        "    bonus_score = 0\n",
        "    # FIXED: Total points should be 1 since there is only one core task.\n",
        "    total_points = 1\n",
        "    feedback = []\n",
        "\n",
        "    # --- Check 1: Homework (Stemming Pipeline) ---\n",
        "    try:\n",
        "        func_exists = 'preprocess_text' in globals()\n",
        "        if func_exists:\n",
        "            test_sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "            correct_output = ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "            student_output = preprocess_text(test_sentence)\n",
        "            if student_output == correct_output:\n",
        "                score += 1\n",
        "                feedback.append(\"- âœ… **Homework (Stemming):** Passed. The preprocessing pipeline function works correctly.\")\n",
        "            else:\n",
        "                feedback.append(f\"- âŒ **Homework (Stemming):** Needs Revision. Your function did not produce the correct output. For the test sentence, expected `{correct_output}` but your function returned `{student_output}`.\")\n",
        "        else:\n",
        "            feedback.append(\"- âŒ **Homework (Stemming):** Failed. The function `preprocess_text` was not found.\")\n",
        "    except Exception as e:\n",
        "        feedback.append(f\"- âŒ **Homework (Stemming):** An error occurred: {e}\")\n",
        "\n",
        "    # --- Bonus Check: (Lemmatization Pipeline) ---\n",
        "    try:\n",
        "        func_exists = 'preprocess_with_lemma' in globals()\n",
        "        if func_exists:\n",
        "            test_sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "            correct_output = ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
        "            student_output = preprocess_with_lemma(test_sentence)\n",
        "            if student_output == correct_output:\n",
        "                bonus_score += 1\n",
        "                feedback.append(\"- ðŸŒŸ **Bonus (Lemmatization):** Passed! The lemmatization pipeline works correctly. Excellent work!\")\n",
        "            else:\n",
        "                feedback.append(f\"- âš ï¸ **Bonus (Lemmatization):** Needs Revision. Found the function, but the output is incorrect. Expected `{correct_output}` but got `{student_output}`.\")\n",
        "    except Exception as e:\n",
        "        # Don't show an error if the bonus wasn't attempted\n",
        "        pass\n",
        "\n",
        "    # --- Final Feedback ---\n",
        "    final_message = \"## **Homework Self-Assessment Feedback**\\n\\n\" + \"\\n\\n\".join(feedback)\n",
        "    final_message += f\"\\n\\n### **Final Score: {score}/{total_points}**\"\n",
        "    if bonus_score > 0:\n",
        "        final_message += f\" (plus **{bonus_score}** bonus point! ðŸŽ‰)\"\n",
        "\n",
        "    if score == total_points:\n",
        "        final_message += \"\\n\\nGreat job! All core tasks passed.\"\n",
        "    else:\n",
        "        final_message += \"\\n\\nSome tasks need revision. Please review the feedback above.\"\n",
        "\n",
        "    display(Markdown(final_message))\n",
        "\n",
        "# Run the fixed self-assessment tool\n",
        "check_week5_nlp_tasks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "cellView": "form",
        "id": "OCVn7S1gLTph",
        "outputId": "24c208e7-24e8-44d4-c686-c1c72faa9720"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## **Homework Self-Assessment Feedback**\n\n- âœ… **Homework (Stemming):** Passed. The preprocessing pipeline function works correctly.\n\n- ðŸŒŸ **Bonus (Lemmatization):** Passed! The lemmatization pipeline works correctly. Excellent work!\n\n### **Final Score: 1/1** (plus **1** bonus point! ðŸŽ‰)\n\nGreat job! All core tasks passed."
          },
          "metadata": {}
        }
      ]
    }
  ]
}