{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Week 6 Homework: Hyperparameter Tuning\n",
        "\n",
        "**Goal**: Step into the role of a machine learning engineer by experimenting with hyperparameters to see how they affect model performance.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_OuUwTRiW3PV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚ñ∂Ô∏è Today's Video\n",
        "\n",
        "If you haven't already, watch this video to understand hyperparameters and why tuning them is one of the most important and creative skills in machine learning.\n",
        "\n",
        "üîó [Neural Networks Summary: All hyperparameters](https://www.youtube.com/watch?v=h291CuASDno)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7svRooOjW8zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîó Neural Networks Summary: All hyperparameters\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# Create the HTML for embedding\n",
        "html_code = f\"\"\"\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/h291CuASDno?si=iko_Nw8BeGjsY0v_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
        "\n",
        "\"\"\"\n",
        "# Display the video\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "cellView": "form",
        "id": "_eF8W9WeW9n6",
        "outputId": "a5189ab4-16f7-4432-bd32-645f2bcc87ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "\n",
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/h291CuASDno?si=iko_Nw8BeGjsY0v_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
              "\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üìñ Today's Theory: The Art of Tuning\n",
        "\n",
        "The values we set ourselves **before training begins**‚Äîlike the **learning rate (`lr`)**, the **number of epochs**, or the **choice of optimizer**‚Äîare called **hyperparameters**. They control the learning process itself.\n",
        "\n",
        "Finding a good combination of hyperparameters is often more of an **art than a science** and is a critical skill for building high-performing models.\n",
        "\n",
        "### üìª Analogy: Tuning a Radio\n",
        "\n",
        "Finding the right learning rate is like tuning an old radio:\n",
        "- Turn the dial **too quickly** (`lr` too high) ‚Üí you overshoot the signal and get noise.\n",
        "- Turn it **too slowly** (`lr` too low) ‚Üí it takes forever to find a clear station.\n",
        "\n",
        "The goal is to find the **sweet spot** where learning is stable and efficient.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Your Task: Experiment, Document, and Analyze\n",
        "\n",
        "Use the **baseline script** (provided below) as your starting point. It‚Äôs a complete, working training and evaluation pipeline for MNIST.\n",
        "\n",
        "### üî¨ The Experiments\n",
        "\n",
        "Run **three separate experiments**. For each:\n",
        "\n",
        "1. Create a **new code cell**.\n",
        "2. Copy the **entire baseline script** into it.\n",
        "3. Make **only the specified change**.\n",
        "4. Run the cell and record the **final test accuracy**.\n",
        "\n",
        "> üí° **Important**: Only change the hyperparameter listed for each experiment. Keep everything else identical.\n",
        "\n",
        "- **Experiment A (High Learning Rate)**: Change `lr` from `0.01` to `0.1`.\n",
        "- **Experiment B (More Epochs)**: Change `epochs` from `3` to `10`.\n",
        "- **Experiment C (Different Optimizer)**: Replace  \n",
        "  `optim.SGD(net.parameters(), lr=0.01, momentum=0.9)`  \n",
        "  with  \n",
        "  `optim.Adam(net.parameters(), lr=0.001)`.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Homework Submission Template\n",
        "\n",
        "At the **very bottom of your notebook**, create a **new Markdown cell** and use this template to document your findings.\n",
        "\n",
        "### My Hyperparameter Tuning Experiments\n",
        "\n",
        "**Experiment A: Learning Rate (0.1)**  \n",
        "- **Final Accuracy**: *What was the accuracy on the test set?*  \n",
        "- **Conclusion**: *How did this high learning rate affect the model's ability to generalize compared to the original model?*\n",
        "\n",
        "**Experiment B: Epochs (10)**  \n",
        "- **Final Accuracy**: *What was the accuracy after 10 epochs?*  \n",
        "- **Conclusion**: *Did training for longer improve performance significantly? What is the trade-off with training time?*\n",
        "\n",
        "**Experiment C: Optimizer (Adam)**  \n",
        "- **Final Accuracy**: *What was Adam's final accuracy?*  \n",
        "- **Conclusion**: *How did Adam's performance compare to SGD's? Which would you choose for this problem and why?*"
      ],
      "metadata": {
        "id": "YqnCOuh-SyO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Week 6 Homework: Hyperparameter Tuning ==========\n",
        "\n",
        "# --- INSTRUCTIONS ---\n",
        "# For each experiment (A, B, C):\n",
        "# 1. Copy this ENTIRE cell into a NEW code cell.\n",
        "# 2. Make ONLY the specified change (see homework instructions).\n",
        "# 3. Run the cell and note the final accuracy.\n",
        "\n",
        "# 1. Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 2. Load Data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 3. Define Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 4. Define Evaluation Function\n",
        "def evaluate_model(model, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# ========== SOLUTION: Baseline (lr=0.01, epochs=3, SGD) ==========\n",
        "# Expected accuracy: ~85-90%\n",
        "\n",
        "# ... (same imports and data loading as above) ...\n",
        "\n",
        "net = Net()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 3\n",
        "# ... rest unchanged ‚Üí accuracy ‚âà 88.5%\n",
        "\n",
        "# ========== SOLUTION: Experiment A (lr=0.1) ==========\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "epochs = 3\n",
        "# ‚Üí Likely unstable, accuracy drops (e.g., ~10-50%)\n",
        "\n",
        "# ========== SOLUTION: Experiment B (epochs=10) ==========\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 10\n",
        "# ‚Üí Accuracy improves slightly (e.g., ~90-92%), but diminishing returns\n",
        "\n",
        "# ========== SOLUTION: Experiment C (Adam, lr=0.001) ==========\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "epochs = 3\n",
        "# ‚Üí Faster convergence, higher accuracy (e.g., ~92-94%)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"üöÄ Starting Training...\")\n",
        "for epoch in range(epochs):\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print(\"üèÅ Finished Training!\")\n",
        "\n",
        "accuracy = evaluate_model(net, testloader)\n",
        "print(f'Final Test Accuracy: {accuracy:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYaaZzwgWYGj",
        "outputId": "5ef9ac46-f38a-4e8e-8d9f-114a3951fb12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Training...\n",
            "üèÅ Finished Training!\n",
            "Final Test Accuracy: 96.04 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment A ‚Äî High Learning Rate (lr = 0.1)"
      ],
      "metadata": {
        "id": "RJE6ITc6FYM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment A: High Learning Rate\n",
        "# I increased the learning rate to 0.1 to test how instability affects performance.\n",
        "\n",
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training setup\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)  # Increased LR\n",
        "epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, loader):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "accuracy = evaluate_model(net, testloader)\n",
        "print(f\"Final Accuracy (High LR): {accuracy:.2f}%\")\n",
        "\n",
        "# The high learning rate (0.1) caused unstable updates and lower accuracy (71.75%).\n",
        "# The model struggled to converge properly due to large parameter jumps."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDlVz0BIFZjM",
        "outputId": "4d3155e9-0df0-4d58-ec13-67b82e16f75b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy (High LR): 71.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment B ‚Äî More Epochs (10)"
      ],
      "metadata": {
        "id": "aTsWSkAfFrgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment B: More Epochs\n",
        "# I increased the number of epochs to allow the model to train longer and improve learning.\n",
        "\n",
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 10  # Increased epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "accuracy = evaluate_model(net, testloader)\n",
        "print(f\"Final Accuracy (10 Epochs): {accuracy:.2f}%\")\n",
        "\n",
        "# Increasing epochs to 10 significantly improved accuracy to 97.30%.\n",
        "# The model had more time to learn, resulting in better convergence but longer training time."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBvNF3j3Fv58",
        "outputId": "fd603f75-d453-47cd-d024-98825f56dc3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy (10 Epochs): 97.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment C ‚Äî Adam Optimizer"
      ],
      "metadata": {
        "id": "qLcU5Q7TGMDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment C: Adam Optimizer\n",
        "# I replaced SGD with the Adam optimizer for faster and more adaptive updates.\n",
        "\n",
        "\n",
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Changed optimizer\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "accuracy = evaluate_model(net, testloader)\n",
        "print(f\"Final Accuracy (Adam): {accuracy:.2f}%\")\n",
        "\n",
        "# The Adam optimizer achieved 96.23% accuracy with faster convergence.\n",
        "# It performed slightly lower than 10-epoch SGD but required less training time."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzxw8aCCGQE1",
        "outputId": "869052d8-ade9-4364-d27d-c7e64240a346"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy (Adam): 96.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning Results**\n",
        "\n",
        "| Experiment                | Hyperparameter Change                           | Training Setup          | Final Accuracy | Observation                                                                            |\n",
        "| ------------------------- | ----------------------------------------------- | ----------------------- | -------------- | -------------------------------------------------------------------------------------- |\n",
        "| **A: High Learning Rate** | Increased learning rate to **0.1**              | 3 epochs, SGD optimizer | **71.75%**     | High learning rate caused unstable updates and poor convergence.                       |\n",
        "| **B: More Epochs**        | Increased epochs to **10**                      | SGD optimizer, lr=0.01  | **97.30%**     | Longer training improved accuracy significantly but required more time.                |\n",
        "| **C: Adam Optimizer**     | Replaced SGD with **Adam** optimizer (lr=0.001) | 3 epochs                | **96.23%**     | Adam achieved fast and stable learning with slightly lower accuracy than 10-epoch SGD. |\n"
      ],
      "metadata": {
        "id": "ikNUQFWDJEMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to Enter your results\n",
        "\n",
        "# ========== Record Your Homework Results ==========\n",
        "# Run this cell to input your experiment accuracies interactively\n",
        "\n",
        "try:\n",
        "    expA = float(input(\"Enter final test accuracy for Experiment A (High LR = 0.1): \"))\n",
        "    expB = float(input(\"Enter final test accuracy for Experiment B (Epochs = 10): \"))\n",
        "    expC = float(input(\"Enter final test accuracy for Experiment C (Adam optimizer): \"))\n",
        "\n",
        "    # Validate ranges\n",
        "    if not all(0 <= acc <= 100 for acc in [expA, expB, expC]):\n",
        "        print(\"‚ö†Ô∏è Warning: Accuracy should be between 0 and 100. Please re-run this cell if values are incorrect.\")\n",
        "\n",
        "    # Store in the expected format for self-assessment\n",
        "    homework_results = {\n",
        "        'expA_acc': expA,\n",
        "        'expB_acc': expB,\n",
        "        'expC_acc': expC\n",
        "    }\n",
        "\n",
        "    print(\"\\n‚úÖ Results saved successfully!\")\n",
        "    print(f\"Experiment A: {expA:.2f}%\")\n",
        "    print(f\"Experiment B: {expB:.2f}%\")\n",
        "    print(f\"Experiment C: {expC:.2f}%\")\n",
        "\n",
        "except ValueError:\n",
        "    print(\"‚ùå Error: Please enter numeric values only (e.g., 85.3). Re-run this cell to try again.\")\n",
        "    homework_results = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "taNL-lKBYSaW",
        "outputId": "1e12fa60-242b-4df6-a28d-fd4f85083c3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter final test accuracy for Experiment A (High LR = 0.1): 71.75\n",
            "Enter final test accuracy for Experiment B (Epochs = 10): 97.30\n",
            "Enter final test accuracy for Experiment C (Adam optimizer): 96.23\n",
            "\n",
            "‚úÖ Results saved successfully!\n",
            "Experiment A: 71.75%\n",
            "Experiment B: 97.30%\n",
            "Experiment C: 96.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Week 6 Homework Self-Assessment ==========\n",
        "\n",
        "#@title Run to check your homework submission\n",
        "from IPython.display import display, Markdown\n",
        "import re\n",
        "\n",
        "def check_homework_submission():\n",
        "    feedback = []\n",
        "    score = 0\n",
        "    total = 1\n",
        "\n",
        "    # Check if a markdown cell with results exists below\n",
        "    # (We can't programmatically read other markdown cells in Colab/Jupyter,\n",
        "    # so we ask the student to define a variable with their results.)\n",
        "\n",
        "    # ALTERNATIVE: Ask student to define a dict in a code cell after experiments\n",
        "    try:\n",
        "        # Student should create this after running all 3 experiments\n",
        "        if 'homework_results' in globals():\n",
        "            results = homework_results\n",
        "            required_keys = {'expA_acc', 'expB_acc', 'expC_acc'}\n",
        "            if not required_keys.issubset(results.keys()):\n",
        "                feedback.append(\"‚ùå Please define `homework_results` with keys: 'expA_acc', 'expB_acc', 'expC_acc'\")\n",
        "            else:\n",
        "                # Basic sanity check: accuracies should be between 0 and 100\n",
        "                valid = all(0 <= v <= 100 for v in [results['expA_acc'], results['expB_acc'], results['expC_acc']])\n",
        "                if valid:\n",
        "                    score += 1\n",
        "                    feedback.append(\"‚úÖ Homework results recorded correctly!\")\n",
        "                else:\n",
        "                    feedback.append(\"‚ùå Accuracy values must be between 0 and 100.\")\n",
        "        else:\n",
        "            feedback.append(\"üìù **Reminder**: After running all 3 experiments, create a code cell with:\\n```python\\nhomework_results = {\\n    'expA_acc': YOUR_ACCURACY_A,\\n    'expB_acc': YOUR_ACCURACY_B,\\n    'expC_acc': YOUR_ACCURACY_C\\n}\\n```\")\n",
        "    except Exception as e:\n",
        "        feedback.append(f\"‚ùå Error checking results: {e}\")\n",
        "\n",
        "    final_message = \"**üéØ Week 6 Homework Self-Assessment**\\n\\n\" + \"\\n\".join(feedback)\n",
        "    final_message += f\"\\n\\nüìä **Score: {score}/{total}**\"\n",
        "    if score == 1:\n",
        "        final_message += \"\\n\\nüéâ Great! You‚Äôve completed the hyperparameter tuning homework. Well done!\"\n",
        "    else:\n",
        "        final_message += \"\\n\\n‚úèÔ∏è Please follow the instructions above to record your results.\"\n",
        "\n",
        "    display(Markdown(final_message))\n",
        "\n",
        "check_homework_submission()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "cellView": "form",
        "id": "Y2knxkMKWunt",
        "outputId": "4e93dee1-8735-4840-e102-880afb105519"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**üéØ Week 6 Homework Self-Assessment**\n\n‚úÖ Homework results recorded correctly!\n\nüìä **Score: 1/1**\n\nüéâ Great! You‚Äôve completed the hyperparameter tuning homework. Well done!"
          },
          "metadata": {}
        }
      ]
    }
  ]
}